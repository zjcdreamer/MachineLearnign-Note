# 2多特征量(多元线性回归)

多特征量（多元线性回归）：  Θ元素有n+1个，x元素有n个，为了方便用矩阵表示方程式，增加一个x0=1，这样两个矩阵维数可以相同。将其中一个矩阵转置，就可以做矩阵乘法

![image-20200402165957073](C:\Users\38246\AppData\Roaming\Typora\typora-user-images\image-20200402165957073.png)

---

# 多元梯度下降法

### 特征缩放

当多个特征之间的取值范围相差较大时，他们的等值线形成的椭圆就会很扁平，梯度下降速度会减慢

![](D:\markdown的笔记\Typora\images\image-20200402203312563.png)

当特征之间的取值范围较小时，收敛的更快，所以为了更快的得到梯度下降的收敛结果，将特征的值进行相应的处理，以尽量保证特征值的取值范围在[-1,1]。

![](D:\markdown的笔记\Typora\images\image-20200402203323048.png)

##### 均值归一化操作：

用Xi - Ui来替代Xi。下图的1000是平均值，2000是范围，即最大值减去最小值

![image-20200402203457472](D:\markdown的笔记\Typora\images\images\image-20200402203323048.png)

特征缩放不要求特别精确，只要尽量使各个特征的取值范围相近就可以了，只是为了使梯度下降的更快



### 学习率

下图：迭代次数和J(Θ)最小值的关系。一般来说，随着迭代次数的不断增加,J(Θ)是在不断变小的，然后就收敛，趋于稳定。这个图可以看出这个算法是否正常（是否随着迭代次数增加，J(Θ)在减小），以及大概迭代多少次时，J(Θ)会收敛

![](D:\markdown的笔记\Typora\images\image-20200402204907005.png)



![](D:\markdown的笔记\Typora\images\image-20200402205230741.png)

如果是上图这两种情况，不断上升或者先下降在上升不断重复，一般是学习率α的值较大，往小更改一下。但是不要让α太小，如果太小的话，梯度下降的就会很慢     

通常建议考虑的学习率：0.01,0.03,0.1,0.3,1,3,10.... 有一个十倍的关系和一个**三倍**的关系



### 特征和多项式回归

如房价预测问题，

![](D:\markdown的笔记\Typora\images\image-20200402212734208.png)
$$
h_θ (x)=θ_0+θ_1×frontage+θ_2×depth 
$$

$$
x_1=frontage（临街宽度），x_2=depth（纵向深度），x=frontage*depth=area（面积），则：
h_θ (x)=θ_0+θ_1 x。
$$

线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：
$$
h_θ (x)=θ_0+θ_1 x_1+θ_2 x_2^2
$$
或者三次方模型：
$$
 h_θ (x)=θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3 
$$
![](D:\markdown的笔记\Typora\images\image-20200402213308633.png)

通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：
$$
x_2=x_2^2,x_3=x_3^3从而将模型转化为线性回归模型。
$$
根据函数图形特性，我们还可以使：
$$
h_θ (x)=θ_0+θ_1 (size)+θ_1 〖(size)〗^2
$$
或者:
$$
h_θ (x)=θ_0+θ_1 (size)+θ_1 √size
$$
**注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。**

---

### 正规方程（区别于迭代方法的直接解法）

之前解决问题一直用的是梯度下降，它需要多次迭代，速度可能会慢一些

正规方程可以一次直接求出Θ，使得代价函数J（θ）最小

![image-20200404215911727](D:\markdown的笔记\Typora\images\image-20200404215911727.png)

当只有一个特征的时候，对J(θ)求导等于0，此时J(θ)值最小

当有多个特征的时候，J(θ1,θ2...θn)不可能让他们一个一个求导，然后等于0。所以直接用正规方程：
$$
θ=(X^T X)^(-1) X^T y
$$
X的转置乘X，再求逆矩阵，然后再乘X的转置乘y

举例：

![image-20200404220331498](D:\markdown的笔记\Typora\images\image-20200404220331498.png)

即：

![image-20200404220353510](D:\markdown的笔记\Typora\images\image-20200404220353510.png)

按照正规方程的求法：

![image-20200404220416936](D:\markdown的笔记\Typora\images\image-20200404220416936.png)

在 **Octave** 中，正规方程写作：

pinv(X'*X)*X'*y

注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。

梯度下降正规矩阵优缺点比较：

| 梯度下降                    | 正规方程                                                     |
| --------------------------- | ------------------------------------------------------------ |
| 需要选择学习率α             | 不需要                                                       |
| 需要多次迭代                | 一次运算得出                                                 |
| 当特征数量n大时也能较好适用 | 需要计算(X^T*X)^(-1) 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n^3)，通常来说当  小于10000 时还是可以接受的 |
| 适用于各种类型的模型        | 只适用于线性模型，不适合逻辑回归模型等其他模型               |

**正规方程的python实现**

```python
import numpy as np
def normalEqn(X, y):
   theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X)
   return theta

```

---

正规方程在矩阵不可逆情况下解决方法

X^T*X不可逆一般是以下两种情况

1. 特征之间不是相互独立的
2. 样本数量少，但是特征多。比如只有10个样本，却有100个特征

