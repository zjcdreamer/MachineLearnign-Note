# 正则化

### 一、过拟合问题

![image-20200407202510630](D:\markdown的笔记\Typora\images\image-20200407202510630.png)

左一：高偏差，欠拟合，不是很好适应样本数据

中间：比较好的拟合图像

右一：高方差，虽然穿过了所有的特征元素，但是图像过于复杂，认为是不稳定的。虽然很好的拟合了我们当前的样本数据，但是当我们再输入一个新的样本数据时，得到的结果并不好。是过拟合的

针对以上的问题，如何处理？

1.  丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. 正则化。 保留所有的特征，但是减少参数的大小（**magnitude**）。

---

### 二、代价函数

从上一节认识到，是因为高此项的存在，使得我们的图像过拟合了。但是还要保留所有的特征变量，这时候就用到了正则化。正则化的思想是：不去除特征变量，而是让高阶特征变量的参数Θ尽可能小(趋向于0)，从而高阶特征变量就相当于失效，从而使我们的函数更简单。

我们要修改代价函数，添加惩罚项，从而正则化。修改后的代价函数如下：

![image-20200407205116351](D:\markdown的笔记\Typora\images\image-20200407205116351.png)

这个1000只是一个很大的值，当这个值很多的时候，还要求整体的最小值，那么就要求参数Θ3、Θ4足够小，就会趋于0。

当我们的特征变量非常多的时候，我们可能不知道哪些特征变量相关性会小一些，因而不知道给哪些参数添加惩罚，所以给所有的参数全都增加惩罚，从而代价函数如下：

![image-20200407205425613](D:\markdown的笔记\Typora\images\image-20200407205425613.png)

λ叫做正则化参数。同时按照惯例，我们只对Θ1到Θn进行正则化，而不对Θ0进行正则化，虽然如果做了结果还是差不多的，但是惯例就是不对Θ0正则化。

当我们的λ过大时，为了是代价函数最小化，就会使得所有参数(除了Θ0)都最小化，也就是都趋向于0，从而使得最后hΘ(x)=Θ0(因为按照惯例不对Θ0进行正则化，那么就只有Θ0幸免于难)，最后得到一条水平的直线，这样我们的函数也是欠拟合的，因此λ的选择也至为关键。

---

### 三、线性回归的正则化

求代价函数的两种方法：**1)、梯度下降**        **2)、正规方程**

#### 梯度下降：

正则化的线性回归表达式：

![image-20200407213137955](D:\markdown的笔记\Typora\images\image-20200407213137955.png)

运用梯度下降算法，分两种情况：

![image-20200407213240902](D:\markdown的笔记\Typora\images\image-20200407213240902.png)

对Θj的表达式进行化简，得到：

![image-20200407213435385](D:\markdown的笔记\Typora\images\image-20200407213435385.png)

可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令                                值减少了一个额外的值。



#### 正规方程

![image-20200407213639608](D:\markdown的笔记\Typora\images\image-20200407213639608.png)



----

### 正则化的逻辑回归模型

![image-20200407215305894](D:\markdown的笔记\Typora\images\image-20200407215305894.png)

```python
import numpy as np
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
```



