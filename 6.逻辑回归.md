# 逻辑回归

### 一、分类

分类：输出离散型结果

比如根据肿瘤的大小判断肿瘤是良性肿瘤还是恶性肿瘤。用(0,1)来分类。一般用0意味着什么都没有，在这里来描述不是恶性肿瘤。1意味着有，在这里代表着恶性肿瘤。

我们将因变量(**dependent variable**)可能属于的两个类分别称为负向类（**negative class**）和正向类（**positive class**），其中 0 表示负向类，1 表示正向类。



![image-20200406164620603](D:\markdown的笔记\Typora\images\image-20200406164620603.png)

尽管训练样本的y都是0或者1，但是如果用线性回归的话，输出值可能远大于1，或者远小于0，这都是不合适的，因此在分类问题中不应用线性回归算法。因此引入逻辑回归算法。**逻辑回归，虽然名字叫回归，但是其实是一种分类算法。**

---

### 假设陈述

上一节用的线性回归来处理的样本数据，但是它并不适合于分类问题。于是引入了逻辑回归算法

逻辑回归是一种分类算法，它的输出值持续保持在0到1之间。逻辑回归模型的假设：
$$
h_θ (x)=g(θ^T X)
$$
其中：X 代表特征向量   g 代表逻辑函数（**logistic function**)是一个常用的逻辑函数为**S**形函数（**Sigmoid function**），公式为：

![image-20200406170550116](D:\markdown的笔记\Typora\images\image-20200406170550116.png)

Sigmoid/logistic函数的图像

![image-20200406170727971](D:\markdown的笔记\Typora\images\image-20200406170727971.png)

综合起来：

![image-20200406170946910](D:\markdown的笔记\Typora\images\image-20200406170946910.png)



对模型的理解：逻辑回归输出的值，就是对当前输入的X而言，y=1的概率

---

### 决策边界

我们预测当hΘ(x)>=0.5的时候，y=1。而当hΘ(x)<0.5的时候，y=0

根据假设函数hΘ(x)和g(z)的关系，可以推出

![image-20200406173419835](D:\markdown的笔记\Typora\images\image-20200406173419835.png)

即：
$$
θ^T X>=0的时候，y=1； θ^T X< 0的时候，y=0
$$


![image-20200406173656248](D:\markdown的笔记\Typora\images\image-20200406173656248.png)

假设Θ0=-3,Θ1=1,Θ2=1，这时候求出Θ的转置，乘以X的矩阵，得到结果：-3+x1+x2。如果让y=1，则-3+x1+x2>=0成立，在图像中可以画出该曲线。这点直线就是决策边界。直线上方是y=1区域，下方是y=0区域。

注意：

​	**决策边界是假设函数的属性，取决于参数Θ，而不是数据集(X)的属性**



![image-20200406174139874](D:\markdown的笔记\Typora\images\image-20200406174139874.png)

当我们遇到更复杂的情况时，可以使用更高阶的多项式，更高阶的多项式可以得出更复杂的决策边界

---

### 代价函数

因为hΘ(x)的特殊性，如果继续使用线性回归的代价函数（带平方），那么就是非凸函数，不能用梯度下降算法得到Θ的最小值。因此要更换代价函数。下图左：非凸函数；图右：凸函数

![image-20200406180318039](D:\markdown的笔记\Typora\images\image-20200406180318039.png)

<img src="D:\markdown的笔记\Typora\images\image-20200406180504785.png" alt="image-20200406180504785" style="zoom:67%;" />![image-20200406180646107](D:\markdown的笔记\Typora\images\image-20200406180646107.png)

<img src="D:\markdown的笔记\Typora\images\image-20200406180718392.png" alt="image-20200406180718392" style="zoom:67%;" />

![image-20200406182517917](D:\markdown的笔记\Typora\images\image-20200406182517917.png)

流程理解：首先根据样本数据的X和Y，带入到代价函数中，这样就剩一个参数Θ了。然后通过梯度下降算法求出使得代价函数J(Θ)最小的Θ，然后就可以求出hΘ(x)了，就有了决策边界



![image-20200406182811767](D:\markdown的笔记\Typora\images\image-20200406182811767.png)

<img src="D:\markdown的笔记\Typora\images\image-20200406182850054.png" alt="image-20200406182850054" style="zoom:67%;" />

---

### 多元分类：一对多