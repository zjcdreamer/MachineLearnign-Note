# 梯度下降

- 梯度下降，就是不断的改变Θ0,Θ1的值，然后求得各种函数的最小值

- 梯度下降的特点：我们讲选取的点发生一些偏移，很可能会找到不同的局部最小值

  ---

  ![image-20200402164840203](C:\Users\38246\AppData\Roaming\Typora\typora-user-images\image-20200402164840203.png)

- α叫做学习效率，它控制着我们以多大的幅度更新参数Θj（α永远是正值）

----

- 梯度下降需要同步更新，右边的那种写法，先给Θ0赋值，那么在算temp1的时候就会带入刚刚算的Θ0，就出来的值就可能变化

  ![image-20200402164921704](C:\Users\38246\AppData\Roaming\Typora\typora-user-images\image-20200402164921704.png)

---

**α过小或过大的情况**

![image-20200402165001078](C:\Users\38246\AppData\Roaming\Typora\typora-user-images\image-20200402165001078.png)

---

- 梯度下降过程中，随着不断的调整，越接近局部最优解，下降的幅度就会越小，因为导数越来越小了
- Batch梯度下降（将梯度下降和线性回归函数结合）之前对应线性回归函数，是我们人工手动来求得的。但是这样太过复杂，所以将梯度下降和线性回归函数结合。每次Θ0和Θ1都会  根据斜率，来自动调节Θ0和Θ1的值，从而自动求得了局部最优解

